# IV Functions of the World Model

&emsp;&emsp;World models play a central role in modern robotics by providing an internal predictive understanding of the environment. They enable robots to reason about future states, anticipate the consequences of actions, and perform counterfactual evaluations, which are particularly valuable in real-world settings where interactions are costly, risky, or time-consuming. By modeling environmental dynamics, world models form the foundation for autonomous, adaptable, and efficient robotic systems. In robotics, world models serve two complementary functions: decision support, by predicting future scenes, actions and planning, and training facilitation, by generating data or acting as learned simulators. These roles are often closely related. For example, a world model used as a simulator can simultaneously generate training data and assist decision making [@lu2025gwm;@liao2025genie]. By combining these functionalities, world models provide a comprehensive framework that enables robots to act intelligently, learn efficiently, and adapt to complex and dynamic environments. Additional details of the world models are provided in Table I, which complements the following discussion.

## A. Decision Support

&emsp;&emsp;1) Implicit World Models for Action Prediction and Planning  
&emsp;&emsp;This line of work explores world models that enable action prediction and planning without explicitly modeling state transitions or world dynamics. These approaches typically leverage the strong reasoning and next-token prediction capabilities of Large Language Models (LLMs), Vision-Language Models (VLMs), and Vision-Language-Action (VLA) models. Since LLMs lack direct access to environmental or robotic states, auxiliary components are often incorporated to provide grounding. For example, Ahn et al. [@ahn2022can] introduce affordance functions to evaluate the feasibility of skills for completing a target task. Xiang et al. [72], [54] employ encoders to process environmental information, while Zhang et al. [60] integrate multimodal tokens including states, images, and text to enhance reasoning and generalization. Zhang et al. [17] further combine 2D and 3D encoders to process RGB images and 3D point clouds, capturing complementary spatial cues for richer world understanding. Hong et al. [18] extend this paradigm by incorporating additional sensory modalities such as vision, audio, tactile, and thermal inputs to achieve a more comprehensive understanding of the environment. 

&emsp;&emsp;Conventional LLMs are language-centric and typically treat visual and other sensory information as auxiliary inputs. VLMs extend this paradigm by jointly learning aligned visual and linguistic representations, enabling grounded perceptual understanding of the world [83]. Zhang et al. [84] further leverage VLMs to generate candidate action sequences, which are evaluated using a lightweight action-conditioned video prediction model to forecast future scenes. The predicted outcomes are then assessed by the VLM to select the final action. An increasing number of studies extend VLMs to VLA by equipping them with low-level action generation capabilities. For instance, Zitkovich et al. [16] represent robot actions as a form of language, effectively bridging perception and control through textual grounding. Zhen et al. [41] employ a 3D-based LLM [85] to represent and predict 3D world states and generate actions, incorporating a diffusion model to synthesize future scenes. Inspired by the dual-process theory of human cognition [102], Bjo ̈rck et al. [44] design a dualsystem architecture in which a VLM serves as the reasoning module (System 2) and a Diffusion Transformer functions as the action module (System 1), with both components jointly optimized for coordinated reasoning and actuation. Zhou et al. [103] preserve the reasoning capability of VLMs while introducing a Mixture-of-Experts (MoE) to alleviate conflicts between multimodal understanding and robotic manipulation in the parameter space. Kim et al. [55] train their model on a large corpus of real-world robot demonstrations, enabling efficient adaptation to new robotic platforms through parameterefficient fine-tuning. 

&emsp;&emsp;To further enhance long-horizon prediction, reasoning, and imagination, several methods integrate large language or multimodal models into other world model architectures, where they serve as core components. For instance, Chen et al. [27] employ the open-source LLM, i.e., InternLM [87], to predict future states from egocentric observations as a fundamental element of the JEPA framework. Similarly, Vision-Language Models [101] and Video-Language Models [86] have been incorporated into Dreamer-style architectures for low-level dynamics modeling, where they extract high-level semantic knowledge of the world to guide prediction. 

&emsp;&emsp;Notably, LLMs, VLMs, and VLAs can also act as explicit world models that predict future scenes [78], [60], [79] or world knowledge [60]. We will elaborate them in Section IV-A3.  

&emsp;&emsp;2) Latent Dynamics Modeling for Action Prediction and Planning  
&emsp;&emsp;This line of research focuses on modeling the temporal evolution of environment dynamics within a latent space, facilitating efficient action prediction, planning and future imagination. Operating in a compact latent space requires fewer environment interactions and reduces computational cost compared to pixel-based modeling. Hafner et al. [29], [12], [13], [14], [15] introduce online planning in latent space through the Recurrent State-Space Model (RSSM), which learns to reconstruct input observations [29]. The Dreamer  series [12], [13], [14], [15] introduces latent imagination, allowing agents to predict and plan over latent trajectories instead of pixels for more efficient and stable policy learning. Specifically, DreamerV1 [12] learns long-horizon behaviors from images by jointly predicting actions and state values, greatly improving sample efficiency. DreamerV2 [13] extends this framework to discrete environments by introducing binary latent variables, achieving human-level performance on the Atari benchmark. DreamerV3 [15], [100] further improves scalability and generative capacity through techniques such as symlog normalization for reward stabilization, refined KL balancing, and enhanced replay buffers. Sekar et al. [39] enhance generalization to downstream tasks through selfsupervised learning without task-specific rewards, while Wu et al. [14] deploy Dreamer in the real world without simulators. Gumbsch et al. [56] introduce context-sensitive dynamics via a context-specific RSSM and hierarchical architecture to improve scalability and long-horizon prediction. Ferraro et al. [25] develop object-centric world models for improved interaction reasoning. 

&emsp;&emsp;Under the JEPA framework, Chen et al. [27] capture causal and temporal dependencies by organizing states and actions into an interleaved sequence, integrating future state prediction and action generation within a unified transformer architecture. Building on this, Assran et al. [77] leverage pre-trained video encoders optimized with a masked denoising objective as the core of JEPA, enabling self-supervised learning through an action-conditioned predictor that autoregressively forecasts future states and actions. Incorporating other potential world models, such as LLMs and VLMs, have been introduced in Section IV-A1. 

&emsp;&emsp;There are also approaches that couple Model Predictive Control (MPC) with learned world models, where the predictive model is used to simulate future trajectories and select optimal actions in a receding-horizon manner. For example, Hansen et al. [104] learn task-specific latent dynamics models using temporal-difference objectives and apply them for efficient online Model Predictive Control. Hansen et al. [105] further improve generalization across diverse embodiments and action spaces by learning an implicit, control-centric dynamics model.  

&emsp;&emsp;3) Vision-based Action Prediction and Planning  
&emsp;&emsp;Vision-based methods enable robots to predict future observations from sensory inputs, allowing them to plan actions in complex and unstructured environments. By simulating sequences of visual outcomes, robots can evaluate longhorizon behaviors, integrate multiple modalities (e.g., vision, language, and control), and generalize to novel tasks without task-specific retraining. This predictive capability makes visual imagination a key component of goal-directed and adaptive robotic decision-making. In particular, action-conditioned multi-frame prediction serves as a crucial element of prediction and planning, allowing robots to mentally simulate the outcomes of different actions before selecting the optimal one for a given task. According to the task formulation, existing approaches can be broadly classified into vision-conditioned and language-conditioned goal representations.  

**Vision-Conditioned Goals. **Finn et al. [36] learn to predict motion dynamics that remain consistent across visual appearances, aiming to enable long-range, action-conditioned video prediction and generalization to unseen objects. Ebert et al. [30], [64] improve long-horizon prediction by using an image registration–based cost function that continuously corrects errors during execution, achieving closed-loop visual planning. Bu et al. [22] further extend this idea with textconditioned video generation to synthesize depth- and flowconsistent sub-goal images. A feedback mechanism then selects sub-goals and generates corresponding actions based on visual error evaluation, bridging visual planning and policy learning. 

&emsp;&emsp;Imagining the future does not inherently produce actions. To enable action predictions. Finn et al. [63], [30], [64] incorporate visual prediction models with model-predictive control (MPC) to select the best action (sequence). Bu et al. [22] use an error-measurement strategy to select the best subgoal images and an MLP that is train with an Inverse Dynamics objective to decode the corresponding actions.  

**Language-Conditioned Goals. **In [36], [63], [30], [64], task specifications are provided as goal images, which are often difficult to obtain and prone to over- or under-specification. To address this limitation, a growing line of research leverages language as a more flexible, compact, and general medium for specifying tasks. However, translating language instructions into precise, actionable representations grounded in the robot’s observations remains challenging due to the misalignment between linguistic descriptions and visual perception. To bridge this gap, Nair et al. [76] use action-conditioned video prediction to simulate future scenes under different action sequences and learn a language-conditioned reward function from crowd-sourced descriptions to measure task completion. The best sequence is selected to maximize the reward. Zhang et al. [78] take advantage of the semantic knowledge and reasoning abilities of VLA and incorporate a decoder into VLA to enable future scene predictions and action generation. Zhou et al. [66] parse language instructions into compositional primitives to capture spatial object relations and generalize to novel commands, while also supporting multimodal task inputs such as goal images and sketches. Zhang et al. [60] enhance reasoning and generalization by introducing additional dream queries that capture historical information and predict dynamic regions, depth, and semantic maps using foundation models such as DINOv2 [106] and SAM [107]. 

**Diverse Goals. **Some works leverage diverse goal conditions to improve task understanding and completion. For instance, Wang et al. [9] develop a language–gesture-conditioned video generation model to disambiguate task specifications and integrate a behavior-cloning policy that unifies visual plan generation and manipulation. Du et al. [59] incorporate observed images as additional context in each frame-denoising step to synthesize video plans and employ an inverse-dynamics model (IDM) to infer the corresponding action sequences. Zhao et al. [79] introduce visual chain-of-thought (CoT) reasoning into VLA models by autoregressively generating sub-goal images alongside language instructions, enabling temporal planning and improving reasoning capability. 

**Action inference. **A key advantage of vision–based action  prediction is that it does not rely on large-scale actionlabeled data. They can be pre-trained on large-scale video data and infer actions by training a simple action extractor using small amounts of action data. Techniques for action extraction include the inverse dynamics model [108], [59], [66], a transformer encoder-decoder architecture [9]. Zhang et al. [7] use vision language models to propose actions, and a tree search to find the best plan. However, video predictions would contain irrelevant information to the target tasks or actions to execute such as background and robot arm. To handle this, Zhi et al. [69] extract 3D Flow from video data and learn 3D optical flow as a representation of object motions to guide action planning. Zhang et al. [60] propose dynamic region-based forecasting, which leverages optical flow prediction model [109], [110] to identify dynamic regions within the scene, enabling the model to concentrate on areas of motion that are critical for task execution instead of redundant frame reconstruction. Agarwal et al. [96] leverage large-scale pre-training on both image and post-training for robotic manipulation, including instruction-based video prediction and action-based next-frame prediction. 3D positional embeddings, including 3D factorized Rotary Position Embedding (RoPE) and absolute positional embedding (APE) for relative positions and absolute coordinates respectively, are adopted to capture spatial and temporal information. Actions are predicted through an action embedder MLP. Tian et al. [61] propose an end-to-end Predictive Inverse Dynamics Models (PIDM), which learn actions and visual futures synergistically to enhance the simulation and action predictions ability. [111] predict both future frames and robot actions within joint latent denoising process, which support planning and acting in a closed-loop manner.  

**Visual Fidelity vs. Action Prediction. **Guo et al. [21] hypothesize that models trained solely with frame-prediction losses tend to emphasize visual appearance fidelity while underestimating accurate dynamics modeling. This highlights the need for approaches that explicitly separate dynamics learning from visual rendering. To address this, FlowDreamer adopts a two-stage framework that first predicts environment dynamics and then renders corresponding visual observations.

## B. Training Facilitation

&emsp;&emsp;World models can act both as data engines, generating synthetic trajectories that support imitation learning and reinforcement learning, and as evaluation modules that provide internal reward estimation or predictive feedback. Because many models combine these roles, it is difficult to assign them to a single category. Accordingly, when discussing each role, we introduce their complementary functions in parallel to highlight this overlap.

&emsp;&emsp;1) Data Engine  
&emsp;&emsp;Large-scale human teleoperation datasets have greatly advanced robot learning [16], [31], [32], [112], [44], [35]. However, collecting such data is labor-intensive and limits coverage across diverse environments and tasks. Vision-based world models, particularly video world models, offer an alternative by learning environment dynamics and generating synthetic data. These models can be broadly divided, according to whether they are conditioned on actions, into static video generation models [38], which predict general future scenes, and action-conditioned video generation models, which simulate how actions change the environment. Beyond data generation, video-based world models increasingly support diverse tasks such as planning, policy learning, and action prediction, which will be reflected in the following content. 

&emsp;&emsp;Specifically, Du et al. [59] target to enable visual world imagination, action planning and generating video demonstration for training by learning a text-conditioned video generation model. Wu et al. [6] train a large-scale video world model to generate accurate and realistic simulated experiences, enabling video prediction, visual planning, and policy training. Jang et al. [38] propose to leverage video world models [97] to generate robot video data. They first fine-tune video world models on a target robot to capture the embodiment-specific dynamics and kinematics and prompt the model with to initial frames and language instruction to generate corresponding data. Pseudo-action labels are generated by means of either a latent action model [113] or an inverse dynamics model (IDM) [114]. Lu et al. [34] leverage 3D-GS reconstruction with Diffusion Transformers (DiTs) to effectively model 3D dynamics, which can promote future scenes generation to support imitation learning and reinforcement learning. Ye et al. [115] synthesize data from diverse perspectives to introduce variations in texture, illumination, viewpoints, physical properties, task diversity, and interaction patterns. Their approach includes: (i) re-rendering real trajectories with diverse visual content, (ii) generating viewpoint-consistent multi-camera scenes with pose adjustments, and (iii) synthesizing embodied interaction sequences, such as converting first-person human videos into robot-centric demonstrations. To ensure realism and avoid hallucination artifacts, the authors further leverage a set of quality assessment metrics that evaluate geometric consistency [116], multiview consistency [116], text–scene alignment [117], and physical plausibility [117]. When constructing world models for training data generation, it is unrealistic to expect any training distribution to encompass all possible configurations of the world. To handle this, Barcellona et al. [33] construct a compositional world model to generate novel demonstration data for training by combining Gaussian Splatting [118] and physics simulators. Equivariant transformation is leveraged to augment data, which modify both observations and the corresponding action sequences to ensure semantical consistency. 

**Support reinforcement learning (RL) based Robotics.**Wang et al. [119] present a video-based world model capable of predicting future visual observations conditioned on VLAgenerated actions. A VLM-guided instant reflector serves as a reward function that quantifies task completion through the semantic alignment between the predicted trajectory and the textual instruction. Despite recent progress, existing methods continue to face challenges in generating diverse and counterfactual data that remain physically plausible, thereby limiting the quality and diversity of synthetic datasets [44]. 

&emsp;&emsp;2) Evaluation  
&emsp;&emsp;Traditionally, robot control policies have been developed and evaluated using handcrafted physics simulators [120], [121], [122]. However, such simulators rely on simplified  or manually engineered dynamics, which struggle to capture complex real-world phenomena, particularly high-DoF interactions, deformable objects, and other non-rigid or contactrich scenarios [123], [124], [125]. Consequently, the resulting discrepancies between simulated and real environments, commonly referred to as the sim-to-real gap, have significantly hindered the deployment and generalization of robotic policies in practice [126], [127]. To handle this, world models potentially emerge as a scalable, reproducible, and informative tool, which reduce reliance on trial-and-error in the real world. Compared to other filed such as autonomous driving [128] and navigation [129], simulated evaluation of robotic manipulation remains difficult because of the highly varied and dynamic interactions that arise between the agent and its environment. Li et al. [130] leverage a video generative world model [97] to produce videos based on action representations from a policy network. A success detector [131] is then used to evaluate task completion from the generated videos and corresponding text prompts. Quevedo et al. [74] evaluate robot polices by means of Monte Carlo rollouts in the world model and take a vision-language model, i.e., GPT-4o [132], as the reward model. He et al. [133] introduce a framelevel control and a motion-reinforced training to improve action-following ability and temporal, dynamic consistency, enhancing the dynamic prediction and action responsiveness of world simulator. More valuable transitions are discovered for policy learning. Zhu et al. [43] construct a frame-level, action-conditioned video world model based on a Diffusion Transformer, enabling scalable policy evaluation, planning, and future-scene generation. Liao et al. [57] take an actionconditioned video generator as the core to model the spatial, temporal, and semantic regularities of real-world interactions that are fundamental to robotic manipulation. The base world model can support future scene generation, action predictions, data engine and closed-loop policy evaluation. Wang et al. [73] promote the versatility of video world models for policy evaluation, visual simulation, synthetic data generation by perform training on heterogeneous actions data with a shared spatial-temporal transformer.  

&emsp;&emsp;Escontrela et al. [134] train an autoregressive transformerbased video prediction model and use the next-token likelihoods of the frozen model as a general reward function across diverse tasks.
[@black2024zero]

\noindent\textbf{Vision-Conditioned Goals.} 
## References
\bibliography