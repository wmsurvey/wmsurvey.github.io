# V Key Techniques and Notable Challenges

 &emsp;&emsp;This section summarizes the key techniques that drive the development of world models and discusses the major challenges that remain in achieving general, scalable, and robust modeling. Some techniques and concepts are revisited across subsections to emphasize their central importance.

## A. Data Limitations

&emsp;&emsp;World models require large amounts of data and supervision to learn generalizable representations of world dynamics and support diverse tasks. However, collecting real-world robotic data is labor-intensive and costly, and the available data are often heterogeneous in format and modality. To overcome these limitations, a variety of strategies have been proposed.

&emsp;&emsp;1) Training Data Scarcity  
&emsp;&emsp;&emsp;a) Leveraging Pre-trained Models.  
&emsp;&emsp;Given the limited availability of training data, many approaches leverage existing pre-trained models. For example, Xiang et al.[@xiang2024pandora] bypass the need for training from scratch by integrating a pre-trained LLM and a pre-trained video model, requiring only lightweight fine-tuning. Zhu et al. [@zhu2025irasim] initialize IRASim with the pre-trained weights of OpenSora [@zheng2024open] to expedite training. Similarly, Sudhakar et al. [@sudhakar2024controlling] leverage a pre-trained diffusion model, while Wang et al. [@wang2025language] utilize Stable Video Diffusion, fine-tuned with robotic videos to adapt to the robotics domain. Song et al. [@song2025physical] further exploit the world knowledge embedded in pre-trained autoregressive video generation models such as NOVA [@deng2025autoregressive].

&emsp;&emsp;&emsp;b) Incorporating Auxiliary Data Sources.  
&emsp;&emsp;Some works tackle the shortage of robot data by using other available sources, such as human manipulation datasets. For instance, Zhi et al. [@zhi20253dflowaction] use both human and robot manipulation videos for training. However, these datasets often contain cluttered backgrounds and similar-looking objects. To address this, they apply optical flow constraints to make the learned representation embodiment-agnostic. Sudhakar et al. [@sudhakar2024controlling] leverage an automatic hand segmentation method to obtain agent-agnostic data for robot learning. Others resort to more diverse data. For example, Yang et al. [@yang2023learning] leverage diverse kinds of data, including objects, scenes, actions, motions, language, and motor control, and convert all actions into a common format.

&emsp;&emsp;&emsp;c) Synthetic Data Generation  
&emsp;&emsp;Instead of relying on real-world data, Deng et al. [@deng2025graspvla] synthesize large-scale action data to train their model. To address the scarcity of 4D data, the Aether team [@team2025aether] generate RGB-D synthetic videos and develop a robust camera-pose annotation pipeline to reconstruct full 4D dynamics. Similarly, Zhen et al. [@zhen2025tesseract] build a 4D embodied video dataset that combines synthetic data with ground-truth depth, normal information and real-world data with estimated depth and normal maps obtained from off-the-shelf estimators.

&emsp;&emsp;2) Heterogeneous action data  
&emsp;&emsp;World models should be able to understand different forms of actions and embodiments to ensure their real-world applications. A basic strategy is to utilize diverse datasets for training. However, the inherent cross-domain and cross-embodiment nature of datasets lead to heterogeneous actions data, including action spaces, action frequencies, and action horizon. For example, diverse embodiment (e.g., different degrees of freedom across robotic arms) and control interface (end effector (EEF) position for arms) would lead to actions of different forms. To handle this, Zheng et al. [@zheng2025universal] learn to capture their shared structural features to obtain the generic atomic behaviors by means of vision language models. Similarly, Zheng et al. [@wang2025learning] lean a share latent space for actions by decoupling observation and actions. More strategies can borrow from relevant fields [@doshi2025scaling;@team2024octo;@wang2024scaling].

&emsp;&emsp;3) Action Label Missing  
&emsp;&emsp;Action-labeled data, which are essential for learning action-conditioned future predictions [@yang2023learning], are particularly scarce in real-world settings.

&emsp;&emsp;&emsp;a) Self-supervised Learning  
&emsp;&emsp;To address the lack of action-labeled data, self-supervised learning techniques have been explored [@finn2016unsupervised;@finn2017deep,ebert2018robustness;@ebert2018visual]. For instance, Finn et al. [@finn2016unsupervised;@finn2017deep] propose to learn pixel-level motion in a self-supervised manner, while Ebert et al. [@ebert2018robustness;@ebert2018visual] leverage image-to-image registration between consecutive video frames to capture dynamics without explicit action labels. However, goal image-based learning presents several drawbacks: such goals are inconvenient for humans to specify, may over-constrain the desired behavior (leading to sparse rewards), or under-specify task-relevant information for non-goal-reaching tasks.

&emsp;&emsp;&emsp;b) Action Label Extraction  
&emsp;&emsp;Another approach to handling missing action labels is to infer them directly from unlabeled videos. More specifically, Bruce et al. [@bruce2024genie;@gao2025adaworld] employ latent action autoencoders to extract latent actions in a self-supervised manner. In their studies, Bruce et al. [@bruce2024genie] sample actions uniformly, while Gao et al. [@gao2025adaworld] introduce biased action sampling to encourage broader exploration and enable action reuse across contexts.Jiang et al. [@jang2025dreamgen] extract pseudo-actions using either a latent action model [@ye2025latent] or an inverse dynamics model (IDM) [@baker2022video]. Du et al. [@du2023learning;@ren2025videoworld;@villar2025playslot;@ko2024learning] learn from unlabeled videos by training inverse dynamics models to infer actions or their embeddings. Ren et al. [@ren2025videoworld] further integrate an inverse dynamics module into a latent dynamics model to leverage rich temporal representations, improving the temporal consistency of predicted actions. Villar et al. [@villar2025playslot] predict latent actions from object-centric representations.

&emsp;&emsp;&emsp;c) Other strategies  
&emsp;&emsp;Some works aim to leverage **pre-trained video models**. For instance, Rigter et al. [@rigter2025avid] adapt a pre-trained video diffusion model for action-conditioned world modeling by training a lightweight adapter, which is then fine-tuned on a small set of domain-specific, action-labeled videos. Black et al. [@black2024zero] similarly employ a pre-trained image-editing diffusion model to support video-based world modeling. In addition, Zhu et al. [@zhu2025unified] design a **unified world model** that integrates the action and video diffusion processes within a unified transformer architecture using separate diffusion timesteps. This can enable learning from action-free video data. Ko et al. [@ko2024learning] utilize **optical flow** extracted from videos, thereby circumventing the need for explicit action labels. 


## B. Perception and Representation

&emsp;&emsp;Perception lies at the heart of robotic world models, enabling systems to interpret task instructions and transform raw sensory inputs into meaningful representations. These representations allow robots to understand structured environments and, in turn, predict, react, and plan effectively.

&emsp;&emsp;1) Inputs  
**Language.** Task instructions are usually given in language. Many methods use pretrained models such as CLIP [@bu2024closed;@ko2024learning;@radford2021learning;@tian2025predictive], Phi [@javaheripi2023phi;@song2025physical], or conditional VAEs [@song2025physical] to extract semantic representations from the instructions.  
**Visual data.** Similarly, visual inputs are often processed using pre-trained visual encoders. For example, Tian et al. [@tian2025predictive] leverage pre-trained Vision Transformers (ViTs) [@he2022masked] to process image observations. Wu et al. [@wu2024ivideogpt] employ a conditional VQGAN that encodes only task-relevant dynamic information, such as the position and pose of moving objects, to reduce temporal redundancy across frames. An autoregressive, GPT-like transformer is then used to generate the next tokens, which are decoded into future frames.  
**Action data.** Actions are sometimes represented as integer values, which lack the contextual richness. This limitation can prevent world models from accurately capturing the intended meaning behind actions. To address this, He et al. [@he2025pre] propose representing actions through language templates that explicitly encode their semantic meaning. In many cases, actions are instead expressed in natural language, as noted above. While this enables richer semantic representations, it also introduces challenges, such as instruction-following ambiguity, which are discussed in Section V-B2a.  
**Diverse data inputs.** Robots need to gain a structured understanding of the world by jointly considering diverse sensory inputs. To achieve this, Song et al. [@song2025physical] embed images and robot actions into a unified physical space, enabling the model to capture the sequential evolution of both the robot and its environment. Hong et al. [@hong2024multiply] incorporate visual, auditory, tactile, and thermal modalities, projecting them into a shared feature space where a language model generates subsequent states and action tokens.

&emsp;&emsp;2) Challenges  
&emsp;&emsp;&emsp;a) Instruction Understanding and Following  
&emsp;&emsp;Instructions convey task goals and can take various forms, including linguistic directives (natural language or structured text), visual cues (sketches, images, or demonstration videos), and others. Compared to image-based goals, textual descriptions provide a more abstract, compositional, and flexible way of specifying objectives, enabling better generalization, clearer intent communication, and more efficient human–robot interaction. Many recent works express target goals through text descriptions [@du2023learning]. Ideally, language instructions should clearly describe the task and remain easily interpretable by the model. However, real-world scenarios often involve ambiguous or novel instructions, making effective interpretation and grounding critical for successful task execution.  
**Ambiguous instructions** In real-world scenarios, language instructions are often ambiguous (e.g., ``put this near here'' [@wang2025language]). To resolve such ambiguity, Wang et al. [@wang2025language] use pointing gestures, interpreted through 2D gripper and object tracking, as an additional instruction modality.  
**New instructions** World models are constrained to make predictions based on language instructions similar to those encountered during training, limiting their ability to generalize to novel commands. To solve this problem, Xiang et al. [@xiang2024pandora] curate a large and diverse set of action-state sequences from re-captioned videos and simulations, and fine-tune world models on this data to improve instruction interpretation and generalize to novel commands and tasks. Li et al.  [@zhou2024robodreamer] employ a text parser to decompose language instructions into primitives, separating actions and spatial relationships. This decomposition allows the model to flexibly recombine these components and generalize to previously unseen combinations of instructions. However, decomposing instructions into primitives can ignore their interrelationships. To address this, Li et al. [@li2025manipdreamer] represent each instruction as an action tree, capturing the hierarchical structure among primitives to better model task organization.

&emsp;&emsp;&emsp;b) Raw Pixels Modeling vs. Concept Abstraction  
&emsp;&emsp;Some studies suggest that humans make predictions based on abstract concepts rather than raw pixels [@chen2025egoagent]. Instead of converting images into discrete tokens [@yang2023learning;@wu2024ivideogpt], Chen et al. [@chen2025egoagent] use learnable convolutional layers to project images into continuous semantic embeddings. Song et al. [@song2025physical] adopt an open-source 3D variational autoencoder (Open-Sora [@zheng2024open]) to obtain video representations. In contrast, another line of work operates directly in pixel space. For instance, Ko et al. [@ko2024learning] adapt a U-Net-based image diffusion model with factorized spatial–temporal convolutions [@dhariwal2021diffusion] to jointly capture spatial and temporal information.

&emsp;&emsp;&emsp;c) Task-irrelevant Issues  
&emsp;&emsp;Visual data often contain information irrelevant to the task, and models such as Vision Transformers (ViTs) may produce hundreds of features per image, affecting both efficiency and effectiveness. To address this, Tian et al. [@tian2025predictive] extract task-relevant features using a perceiver resampler~\cite{alayrac2022flamingo}. Ren et al. [@ren2025videoworld] learn compact visual representations that preserve fine-grained temporal dynamics through a causal encoder–decoder structure and quantization with a discrete codebook [@mentzer2024finite].

&emsp;&emsp;&emsp;d) Spatiotemporal Awareness  
&emsp;&emsp;Understanding the world requires modeling how spatial structures evolve over time. To this end, several works design architectures that explicitly capture spatial and temporal dependencies. Tian et al. [@tian2025predictive] enhance token representations with learnable positional embeddings at each timestep to capture temporal information. Bruce et al. [@bruce2024genie] develop a spatiotemporal transformer composed of multiple spatiotemporal blocks to model spatial–temporal relationships in dynamic scenes. Ko et al. [@ko2024learning] adopt factorized spatiotemporal convolutions following the design of [@ho2022video]. Zhang et al. [@zhang2025dreamvla] extract spatiotemporal patch representations using a masked autoencoder [@he2022masked]. Other studies incorporate additional cues to better understand the three-dimensional structure of the environment. For example, Zhang et al. [@zhang2025dreamvla] estimate depth information using depth estimation techniques [@yang2024depth] to enhance 3D spatial understanding. When encoding multi-view inputs, Liao et al. [@liao2025genie] augment each token with 2D rotary positional embeddings, view-specific learnable embeddings, and timestep encodings to promote spatiotemporal alignment while preserving viewpoint-specific distinctions.

<figure markdown>
  ![Perspectives on world models](assets/img/05-01.png){ width="60%" }
</figure>

## C. Long-horizon Reasoning  
&emsp;&emsp;Many robotic tasks require coherent long-horizon reasoning, where achieving the final objective depends on executing a temporally consistent sequence of actions over extended time scales. Existing methods are limited in long-horizon predictions  [@nair2022learning;@ha2018world;@hafner2019learning;@hafner2021mastering;@hafner2023mastering]. For example, Ha et al. [@ha2018world;@hafner2019learning;@hafner2021mastering;@hafner2023mastering] predefine temporal horizons to guide planning in their world models. In terms of video generation, existing methods still suffer from limited length (short-horizon future video) [@gao2024flip]. For example, Ko et al. [@ko2024learning] predicts a fixed number (eight) of future frames with U-Net based diffusion model  [@dhariwal2021diffusion]. Bruce et al. [@bruce2024genie] can only memorize 16 frames and cannot produce consistent predictions. For autoregressive models, small prediction errors compound sequentially, leading to substantial inaccuracies in long-horizon forecasts.

&emsp;&emsp;1) Closed-loop learning scheme  
&emsp;&emsp;A line of work enabling long-term planning/predictions by learning through interaction with feedback and adjusting their behaviour accordingly [@du2023video;@bu2024closed]. For example, Ebert et al. [@ebert2018robustness;@ebert2018visual] utilize image-to-image registration between predicted video frames and both the start and the goal images with the average length of the warping vectors as a cost function. The model would continue to retry until the task is completed. Du et al. [@du2023video] proposes a recursive planning framework comprising action proposal, video rollout generation, and evaluation. Vision–language models (VLMs) are used to propose potential next actions, while video generation models simulate multiple possible future rollouts. The resulting trajectories are then evaluated by the VLMs to select the optimal action. Du et al. [@liao2025genie] design a neural simulator that predicts future visuals, enabling policy models to interact within a consistent environment. A sparse memory mechanism is leveraged to further enhance the consistency over the time. 

&emsp;&emsp;2) Subgoals  
&emsp;&emsp;Pre-trained models possess a vast repository of commonsense and procedural knowledge that can be leveraged to decompose a high-level goal, often specified in natural language (e.g., "make a cup of coffee"), into a logical sequence of concrete sub-goals or skills. Bu et al. [@bu2024closed] propose to promote long-horizon manipulation tasks by decomposing the goal into sub-goals and handling error accumulations by designing a real-time feedback mechanism. Yang et al. [@yang2025roboenvision] leverage VLM to produce sub-goals and utilize coarse and fine video diffusion models to generate long-horizon videos. Chen et al. [@chen2025robohorizon] utilizes an LLM to generate a multi-stage plan and design a LLM-based dense reward generator for sub-tasks, providing crucial guidance for long-horizon planning. 

&emsp;&emsp;3) Hierarchical structures  
&emsp;&emsp;Bu et al. [@gumbsch2023learning] propose hierarchical world models with Adaptive Temporal Abstractions that separate the modeling of dynamics into high-level and low-level latent states. The low-level model captures fine-grained, short-term dynamics for immediate reactions, while the high-level model abstracts over longer temporal horizons to represent extended dependencies and long-term goals. By dynamically adapting the temporal granularity of the high-level latent states, the model can efficiently plan and predict over long horizons while maintaining accurate short-term predictions through the low-level module.

&emsp;&emsp;4) More strategies.  
&emsp;&emsp;Driess et al. [@driess2023palm] provide a goal image in addition to language instructions. Du et al. [@du2023video] propose to take advantage of long-horizon inference of VLMs and the low-level visual dynamic modelling ability of text-to-video models to handle long-horizon visual planning. A tree search over the space of possible video sequences to find proper long-horizon plans. Ren et al. [@ren2025videoworld] lean compact representations for the visual world that preserve the detailed temporal dynamics by means of causal encoder-decoder and quantization with a discrete codebook [@mentzer2024finite]. 

## D. Spatiotemporal Consistency  
Spatiotemporal consistency plays a vital role in ensuring coherent and physically plausible predictions of future states. It guarantees that the model preserves object continuity, motion smoothness, and causal relationships across time, enabling stable video simulation and reliable dynamics forecasting.

&emsp;&emsp;1) Data perspective  
&emsp;&emsp;In conditional video synthesis, Du et al. [@du2023learning] incorporates the observed image as additional context when denoising each frame. Specifically, it adapts a temporal super-resolution diffusion architecture by tiling the conditioned visual observation across all timesteps. Each intermediate noisy frame is concatenated with the observed image throughout sampling, providing a strong spatial anchor that enforces consistent environmental states across time. Ko et al. [@ko2024learning] concatenates the initial condition frame with all subsequent frames, providing a stable reference that preserves both the spatial layout and temporal evolution of the environment throughout the sequence. Zhen et al. [@zhen2025tesseract] refine depth maps using normal integration to enhance spatial consistency. Optical flow is then calculated to ensure depth coherence across frames, maintaining consistent scene geometry over time. 

&emsp;&emsp;2) Model perspective  
&emsp;&emsp;Yang et al. [@yang2025roboenvision] noted that in autoregressive predictions, standard spatiotemporal attention in video diffusion models degrades frame consistency due to limited long-range context. To address this, the temporal attention layers are replaced with 3D full attention layers, enabling computation of attention across all spatiotemporal tokens and better modeling of large motions. Additionally, the spatial attention layers are modified by reinjecting the VAE features of the first frame and computing cross-attention with the spatial tokens of the query features, further enhancing frame coherence.

&emsp;&emsp;3) Memory mechanism  
&emsp;&emsp;is often used to enhance the spatiotemporal consistency. For example, Liao et al. [@liao2025genie] design a sparse memory mechanism to provide long-term historical context, improving spatiotemporal consistency and task relevance. More information can refer to Section V-G.


## V-E Generalization
## V-F Physics-informed Learning
## V-G Memory
## V-H Other Challenges

## References
\bibliography