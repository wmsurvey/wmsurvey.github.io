# **VII Dataset**

&emsp;&emsp;There are abundant datasets that facilitate robot learning, including general robotic manipulation datasets [@khazatsky2024droid;@ebert2018robustness;@walke2023bridgedata;@lynch2023interactive;@ebert2021bridge;@mandlekar2019scaling;@deng2025graspvla;@chen2024rh20t], dual-arm robotic manipulation datasets [@bu2025agibot],  human manipulation datasets [@miech2019howto100m;@goyal2017something;@souvcek2024genhowto;@carreira2019short;@cheang2024gr;@zhen20243d], combinations of robotic \& human manipulation [@zhen20243d], egocentric datasets [@grauman2022ego4d;@grauman2024ego;@damen2018scaling], 3D \& 4D datasets [@zhen2025tesseract;@zhen20243d;@mandlekar2019scaling;@zhi20253dflowaction;@grauman2022ego4d;@grauman2024ego;@ramakrishnan22021habitat;@chang2017matterport3d;@hong2024multiply;@o2024open;@gu2023maniskill2],  multi-view datasets [@hong2024multiply;@grauman2024ego;@chen2024rh20t] and panoramic-view datasets [@chang2017matterport3d]. A detailed information  can be found in Table. Ⅱ.

### Table Ⅱ {style="text-align:center"}

A SUMMARY OF REPRESENTATIVE DATASETS. `H`: HOUR, `Manip`: MANIPULATION, `Env.`: ENVIRONMENTS, `Traj.`: TRAJECTORIES. 
<figure markdown>
  <!-- ![Perspectives on world models](assets/img/table1title.png){ width="100%" } -->
  ![Perspectives on world models](assets/img/table2.png){ width="100%" }
</figure>

&emsp;&emsp;Recent efforts in world models for robotic manipulation have leveraged large and diverse datasets, often combining multiple sources, to enable generalization across tasks and environments. For example, Yang *et al.* [@yang2023learning] construct a large-scale natural dataset combining simulated executions and renderings [@ramakrishnan22021habitat;@lynch2023interactive], real robot data [@ebert2021bridge], human activity videos [@grauman2022ego4d;@damen2018scaling;@goyal2017something], 3D panorama scans [@chang2017matterport3d], and internet text-image data (i.e., LAION-400M [@schuhmann2021laion]). Bruce *et al.* [@bruce2024genie] combine the RT-1 dataset [@brohan2023rt] with real robot grasping data [@kalashnikov2018scalable]. Wu *et al.* [@wu2024ivideogpt] train the world model based on the combination of the Open X-Embodiment (OXE) dataset [@o2024open] and the Something-Something v2 trajectory dataset [@goyal2017something]. Bruce *et al.* [@cheang2024gr] employ a pretraining and fine-tuning strategy. In the pretraining stage, a combination of human demonstration datasets such as Howto100M [@miech2019howto100m], Ego4D [@grauman2022ego4d], Something-Something V2 [@goyal2017something], EPIC-KITCHENS [@damen2018scaling], Kinetics-700 [@carreira2019short], and robot datasets [@brohan2023rt;@walke2023bridgedata]. Fine-tuning data includes 105 table-top tasks via teleoperation covering eight skills (e.g., pick, place). Data augmentations are performed to add new objects or change backgrounds by means of a diffusion model [@ho2020denoising] and the Segment Anything Model (SAM) [@kirillov2023segment], as well as a video generation model [@ma2025latte] to synthesize new videos. Du *et al.* [@du2023learning] curate an internet-scale pretraining dataset consisting of 14 million video-text pairs, 60 million image-text pairs [@ho2022imagen], LAION-400M [@schuhmann2021laion], and a smaller real-world robotic dataset [@ebert2021bridge]. Huang *et al.* [@huang2025enerverse] construct multi-anchor view video datasets using public sources including RT-1 [@brohan2023rt], Taco-Play [@rosete2023latent], ManiSkill [@gu2023maniskill2], BridgeData V2 [@walke2023bridgedata], LanguageTable [@lynch2023interactive], and RoboTurk [@mandlekar2019scaling], augmented with Isaac Sim simulations [@mittal2023orbit].  [@zhen2025tesseract] construct a 4D embodied video dataset based on previous datasets [@james2020rlbench;@brohan2023rt;@walke2023bridgedata;@goyal2017something] by measuring depth and normal information.

## **References**
\bibliography