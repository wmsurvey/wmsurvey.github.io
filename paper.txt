Traditionally, robot control policies have been developed and evaluated using handcrafted physics simulators \cite{todorov2012mujoco,erez2015simulation,tedrake2019drake}. However, such simulators rely on simplified or manually engineered dynamics, which struggle to capture complex real-world phenomena, particularly high-DoF interactions, deformable objects, and other non-rigid or contact-rich scenarios \cite{sunderhauf2018limits,afzal2020study,choi2021use}. Consequently, the resulting discrepancies between simulated and real environments, commonly referred to as the sim-to-real gap, have significantly hindered the deployment and generalization of robotic policies in practice \cite{dulac2019challenges,zhao2020sim}. To handle this, world models potentially emerge as a scalable, reproducible, and informative tool, which reduce reliance on trial-and-error in the real world. Compared to other filed such as autonomous driving \cite{dosovitskiy2017carla} and navigation \cite{deitke2020robothor}, simulated evaluation of robotic manipulation remains difficult because of the highly varied and dynamic interactions that arise between the agent and its environment. Li \textit{et al.}~\cite{li2025worldeval} leverage a video generative world model~\cite{wan2025wan} to produce videos based on action representations from a policy network. A success detector \cite{team2023gemini} is then used to evaluate task completion from the generated videos and corresponding text prompts. Quevedo \textit{et al.} \cite{quevedo2025evaluating} evaluate robot polices by means of Monte Carlo rollouts in the world model and take a vision-language model, i.e., GPT-4o \cite{hurst2024gpt}, as the reward model. He \textit{et al.} \cite{he2025pre} introduce a frame-level control and a motion-reinforced training to improve action-following ability and temporal, dynamic consistency, enhancing the dynamic prediction and action responsiveness of world simulator. More valuable transitions are discovered for policy learning. Zhu \textit{et al.} \cite{zhu2025irasim} construct a frame-level, action-conditioned video world model based on a Diffusion Transformer, enabling scalable policy evaluation, planning, and future-scene generation. Liao \textit{et al.} \cite{liao2025genie} take an action-conditioned video generator as the core to model the spatial, temporal, and semantic regularities of real-world interactions that are fundamental to robotic manipulation. The base world model can support future scene generation, action predictions, data engine and closed-loop policy evaluation. Wang \textit{et al.} \cite{wang2025learning} promote the versatility of video world models for policy evaluation, visual simulation, synthetic data generation by perform training on heterogeneous actions data with a shared spatial-temporal transformer.

Escontrela \textit{et al.} \cite{escontrela2023video} train an autoregressive transformer-based video prediction model and use the next-token likelihoods of the frozen model as a general \textbf{reward} function across diverse tasks.
