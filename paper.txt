Robots acquire skills by observing and imitating humans or other agents in their environment. Depending on the observation viewpoint, world models for robot learning can be categorized into third-person (exocentric) \cite{guo2025flowdreamer,ferraro2025focus,villar2025playslot} and first-person (egocentric) \cite{chen2025egoagent,grauman2024ego} perspectives. Many existing methods learn from exocentric perspectives, capturing skills from an external viewpoint \cite{guo2025flowdreamer,ferraro2025focus,villar2025playslot}. However, exocentric observations do not fully align with how humans perceive the world. This has motivated the development of egocentric world models. For example, Chen \textit{et al.} \cite{chen2025egoagent} observe a continuous loop of human interactions, in which humans perceive egocentric observations and take 3D actions repeatedly. They model interactions as sequences of ``state-action-state-action'' tokens, processed using a causal attention mechanism. Zhang \textit{et al.} \cite{zhang2025combo} focus on multi-agent planning, inferring other agents' actions from world states estimated via partial egocentric observations.

Grauman \textit{et al.} \cite{grauman2024ego} argue that egocentric and exocentric viewpoints are complementary. Learning through egocentric viewpoints allows robots to better understand hand-object interactions and the attention mechanism of the camera wearer, while exocentric perspectives provide information about the surrounding environment and whole-body poses.