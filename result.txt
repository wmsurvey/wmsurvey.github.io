&emsp;&emsp;Robots acquire skills by observing and imitating humans or other agents in their environment. Depending on the observation viewpoint, world models for robot learning can be categorized into third-person (exocentric) [@guo2025flowdreamer;@ferraro2025focus;@villar2025playslot] and first-person (egocentric) [@chen2025egoagent;@grauman2024ego] perspectives. Many existing methods learn from exocentric perspectives, capturing skills from an external viewpoint [@guo2025flowdreamer;@ferraro2025focus;@villar2025playslot]. However, exocentric observations do not fully align with how humans perceive the world. This has motivated the development of egocentric world models. For example, Chen *et al.* [@chen2025egoagent] observe a continuous loop of human interactions, in which humans perceive egocentric observations and take 3D actions repeatedly. They model interactions as sequences of ``state-action-state-action'' tokens, processed using a causal attention mechanism. Zhang *et al.* [@zhang2025combo] focus on multi-agent planning, inferring other agents' actions from world states estimated via partial egocentric observations.

&emsp;&emsp;Grauman *et al.* [@grauman2024ego] argue that egocentric and exocentric viewpoints are complementary. Learning through egocentric viewpoints allows robots to better understand hand-object interactions and the attention mechanism of the camera wearer, while exocentric perspectives provide information about the surrounding environment and whole-body poses.