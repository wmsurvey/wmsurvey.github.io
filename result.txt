&emsp;&emsp;Traditionally, robot control policies have been developed and evaluated using handcrafted physics simulators [@todorov2012mujoco;@erez2015simulation;@tedrake2019drake]. However, such simulators rely on simplified or manually engineered dynamics, which struggle to capture complex real-world phenomena, particularly high-DoF interactions, deformable objects, and other non-rigid or contact-rich scenarios [@sunderhauf2018limits;@afzal2020study;@choi2021use]. Consequently, the resulting discrepancies between simulated and real environments, commonly referred to as the sim-to-real gap, have significantly hindered the deployment and generalization of robotic policies in practice [@dulac2019challenges;@zhao2020sim]. To handle this, world models potentially emerge as a scalable, reproducible, and informative tool, which reduce reliance on trial-and-error in the real world. Compared to other filed such as autonomous driving [@dosovitskiy2017carla] and navigation [@deitke2020robothor], simulated evaluation of robotic manipulation remains difficult because of the highly varied and dynamic interactions that arise between the agent and its environment. Li *et al.* [@li2025worldeval] leverage a video generative world model [@wan2025wan] to produce videos based on action representations from a policy network. A success detector [@team2023gemini] is then used to evaluate task completion from the generated videos and corresponding text prompts. Quevedo *et al.* [@quevedo2025evaluating] evaluate robot polices by means of Monte Carlo rollouts in the world model and take a vision-language model, i.e., GPT-4o [@hurst2024gpt], as the reward model. He *et al.* [@he2025pre] introduce a frame-level control and a motion-reinforced training to improve action-following ability and temporal, dynamic consistency, enhancing the dynamic prediction and action responsiveness of world simulator. More valuable transitions are discovered for policy learning. Zhu *et al.* [@zhu2025irasim] construct a frame-level, action-conditioned video world model based on a Diffusion Transformer, enabling scalable policy evaluation, planning, and future-scene generation. Liao *et al.* [@liao2025genie] take an action-conditioned video generator as the core to model the spatial, temporal, and semantic regularities of real-world interactions that are fundamental to robotic manipulation. The base world model can support future scene generation, action predictions, data engine and closed-loop policy evaluation. Wang *et al.* [@wang2025learning] promote the versatility of video world models for policy evaluation, visual simulation, synthetic data generation by perform training on heterogeneous actions data with a shared spatial-temporal transformer.

&emsp;&emsp;Escontrela *et al.* [@escontrela2023video] train an autoregressive transformer-based video prediction model and use the next-token likelihoods of the frozen model as a general **reward** function across diverse tasks.
