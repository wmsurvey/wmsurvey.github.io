## **Generalization**

&emsp;&emsp;Robots are expected to operate robustly in complex and novel environments, interacting with unfamiliar objects and performing tasks beyond their training distribution.

&emsp;&emsp;**) Data Scaling**

&emsp;&emsp;An intuitive and effective strategy to enhance generalization is to scale the diversity and volume of training data. For example, Cheang *et al.* [@cheang2024gr] increase the number of pre-training videos from 0.8 million in [@wu2024unleashing] to 38 million. Assran *et al.* [@assran2025v] expand the dataset from 2 million used by [@bardes2024revisiting] to 22 million videos. Wang *et al.* [@wang2025learning] expand each of the 40 datasets by increasing trajectories from 10 up to $10^{6}$. Cheang *et al.* [@cheang2025gr] train the model with web-scale vision-language data,  human trajectory data and robot trajectory data. Kevin *et al.* [@intelligence2025pi_] leverage diverse mobile manipulator data, diverse multi-environment non-mobile robot data, cross-embodiment laboratory data, high-level subtask prediction, and multi-modal web data. Cheang *et al.* [@barcellona2025dream;@cheang2024gr] investigate **data augmentation** strategies to enhance generalization. In [@barcellona2025dream], object rotation and roto-translation are applied. Cheang *et al.* [@cheang2024gr] generate novel scenes by injecting objects using a diffusion model [@ho2020denoising] and/or altering backgrounds with the Segment Anything Model (SAM)  [@kirillov2023segment]. A video generation model [@kirillov2023segment] is subsequently employed to synthesize videos that preserve the original robot motions from the inpainted frames. Liao *et al.*   [@liao2025genie] augment the dataset with a diverse set of failure cases, including erroneous executions, incomplete behaviors, and suboptimal control trajectoriesâ€”collected from both human teleoperation and real-world robotic deployments. One problem of data scaling is that it is unlikely to collect all data for each tasks. At the same time, how to balance different data tasks is also challenging. Moreover, performance gains by scaling data is also limited for consistent performance improvements.

&emsp;&emsp;**) Use of Pretrained Models**

&emsp;&emsp;Many methods aim to enhance generalization by leveraging the generative capabilities of video models. For example, Zhu *et al.* [@team2025aether] combine video generation with geometric-aware learning to improve synthetic-to-real generalization across unseen viewpoints and support multiple downstream tasks. Zhen *et al.* [@zhen2025tesseract] fine-tune a video generation model on RGB, depth, and normal videos to encode detailed shape, configuration, and temporal dynamics, enabling generalization to unseen scenes, objects, and cross-domain scenarios. The generalization capabilities of large language models, such as video-language models [@wang2025founder] and vision-language models [@mazzaglia2024genrl], can be leveraged to enhance world models. By extracting high-level knowledge about the environment, these models facilitate more effective low-level dynamics modeling.

&emsp;&emsp;**) Instructions Decomposing**

&emsp;&emsp;Another generation issue comes from unseen instructions. To handle this, Zhou *et al.* [@zhou2024robodreamer] enhance the ability to unseen instructions by decomposing each spatial relation phrase into a set of compositional components with the pre-trained parser [@kitaev2019multilingual] and the rule-based approach. Detailed information can refer to Section **@@@**.

&emsp;&emsp;**) Invariant Representations**

&emsp;&emsp;Generalization can be significantly improved by learning invariant representations to superficial or task-irrelevant changes in the environment. For example, Pang *et al.* [@pang2025reviwo] model learns to explicitly decompose visual observations into a view-invariant representation, which is used for the control policy, and a view-dependent representation. This decoupling makes the resulting policy robust to changes in camera viewpoint, a common source of failure in visuomotor control. Similarly, the Martinez *et al.* [@martinez2025coral] framework learns a transferable communicative context between two agents, which enables zero-shot adaptation to entirely unseen sparse-reward environments by decoupling the representation learning from the control problem. Wu *et al.* [@wu2023pre] disentangle the modeling of context and dynamics by introducing a context encoder, enabling the model to capture shared knowledge for predictions.

&emsp;&emsp;**) Task-relevant Information Focused**

&emsp;&emsp;Video data often contain irrelevant data to the actions such as background and robot arm, which would limited the generalization ability of the learned world models. To handle this, [@zhi20253dflowaction] propose to object-centric world models, which concentrated on object movements via the optical flow predictions that is independent of embodiment. Finn *et al.* [@finn2016unsupervised] propose to explicitly model and predict motion that are relatively invariant to the object appearance, enabling long-range predictions and generalize to unseen objects.

&emsp;&emsp;**) Other Strategies**

&emsp;&emsp;Black *et al.* [@black2024zero] use a pretrained image-editing model to generate subgoals from language commands and current observations, enabling low-level controllers to act and generalize to novel objects and scenarios. Self-supervised learning without task-specific rewards that can enhancing generalization abilities into different tasks [@sekar2020planning].

## **Physics-informed Learning**

&emsp;&emsp;Existing world models struggle to generate physically consistent videos because they lack an inherent understanding of physics, often producing unrealistic dynamics and implausible event sequences. Simply scaling up training data or model size is insufficient to capture the underlying physical laws [@kang2025far]. To address this challenge, several approaches have been proposed.
&emsp;&emsp;For example, Yang *et al.* [@yang2025vlipp] introduce a two-stage image-to-video generation framework that explicitly incorporates physics through vision- and language-informed physical priors. Team *et al.* [@team2025aether] estimate depth and camera pose directly from videos, facilitating physics-informed learning and enabling world models to infer and predict physically consistent dynamics. Peper *et al.* [@peper2025four] argue that advancing from physics-informed to physics-interpretable world models requires rethinking model design, and propose four guiding principles: organizing latent spaces by physical intent, encoding invariant and equivariant environmental representations, integrating multiple supervision signals, and partitioning generative outputs to improve both scalability and verifiability.